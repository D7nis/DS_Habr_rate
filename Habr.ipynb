{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import csv\n",
    "import pymorphy2\n",
    "import datetime\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.pool import NullPool\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.field_size_limit(231072);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.field_size_limit(int(sys.maxsize/10000000000));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к. датасет большой, то буду загружать данные порциями, очищать, нормализовать и генерировать новые фичи.\n",
    "Подготовленные данные запишу в базу данных и буду работать с ней."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все необходимые функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanDataFirstSatge (df):\n",
    "    '''\n",
    "    Удалю статьи, состоящие только из одного слова/символа или цифр.\n",
    "    Так же статьи с пустым полем 'text'\n",
    "    '''\n",
    "    df_for_drop = df[((df['text'].apply(lambda t: len(t.strip().split()))<=1)\\\n",
    "      & (df['tittle'].apply(lambda t: len(t.strip().split()))<=1))\\\n",
    "      | (df['text'] == '   ')]\n",
    "    return df_for_drop.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddNewFeature_dates(df):\n",
    "    '''\n",
    "    Генерация фич на основе времени\n",
    "    '''\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['month'] = df['date'].dt.month \n",
    "    df['day'] = df['date'].dt.day \n",
    "    df['hour'] = df['date'].dt.hour \n",
    "    df['weekday'] = df['date'].dt.dayofweek\n",
    "    #df.sort_values(by='date',ascending=True,inplace=True)\n",
    "    \n",
    "    return df['date'],df['month'],df['day'],df['hour'],df['weekday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSites (txt):  \n",
    "    sites = re.findall(r'[\\w]+\\.{1,1}[^\\.\\d^\\s]\\w{1,3}\\b',str(txt)) #сайты\n",
    "    return (' '.join(sites)).replace('.','_'),len(sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetEnWords (txt):  \n",
    "    en_words = re.findall(r'\\b[A-Za-z]{3,}\\b',re.sub(r'[A-Za-z]{2,}\\.+[A-Za-z]*\\b','',str(txt))) #слова на латинице от 3х букв    \n",
    "    return ' '.join(en_words),len(en_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetRuWords (txt):  \n",
    "    ru_words = re.findall(r'\\b[А-Яа-я]{4,}\\b',re.sub(r'[А-Яа-я]{2,}\\.+[А-Яа-я]*\\b','',str(txt))) #слова на кириллице от 4х букв\n",
    "    return ' '.join(ru_words),len(ru_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetPunctuationsCount (txt):  \n",
    "    punctuations  = re.findall(r'[^\\w\\s]',txt) #Знаки препинания\n",
    "    return len(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetEmotionsCount (txt):  \n",
    "    emotions  = re.findall(r'[\\?]{2,}|[\\!]{2,}',txt) #Знаки ??? , !!!\n",
    "    return len(emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAllWordsCount (txt):  \n",
    "    words_count = len(txt.strip().split()) #Количество текстовых объектов\n",
    "    clwords  = re.findall(r'[А-ЯA-Z]{1,}[а-яa-z]{1,}\\b',txt) #Количество слов с заглавной буквы\n",
    "    return words_count,len(clwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddNewFeatures_from_txt(columns):\n",
    "    for index, row in df.iterrows():\n",
    "        df.loc[index, 'sites_'+column],df.loc[index, 'sites_'+column+'_count'] = GetSites(row[column])\n",
    "        df.loc[index, 'ru_words_'+column],df.loc[index, 'ru_words_'+column+'_count'] = GetRuWords(row[column])\n",
    "        df.loc[index, 'en_words_'+column],df.loc[index, 'en_words_'+column+'_count'] = GetEnWords(row[column])\n",
    "        df.loc[index, 'punctuations_'+column+'_count'] = GetPunctuationsCount(row[column])\n",
    "        df.loc[index, 'emotions_'+column+'_count'] = GetEmotionsCount(row[column])\n",
    "        df.loc[index, column+'_words_count'],df.loc[index, column+'_clwords_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddNewFeature_text_category(df):\n",
    "    '''\n",
    "    Создание фичи категория текста (размер)\n",
    "    '''\n",
    "    twc25 = df['text_words_count'].describe()['25%']\n",
    "    twc50 = df['text_words_count'].describe()['50%']\n",
    "    twc75 = df['text_words_count'].describe()['75%']\n",
    "    twcmax = df['text_words_count'].describe()['max']\n",
    "    \n",
    "    df['text_category'] = pd.cut(df.text_words_count\\\n",
    "                             ,bins=[-1,0,twc25,twc50,twc75,twcmax]\\\n",
    "                             ,labels=['0','1','2','3','4']) \n",
    "    return df['text_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddNewFeature_rate_category (df):\n",
    "    #Разделю рейтинги на 3 категории (отрицательный, нейтральный и положительный)\n",
    "    df_rate_count = df['rate'].value_counts().rename('count').reset_index()\n",
    "    df_rate_count.rename({'index':'rate'}, axis='columns',inplace=True)\n",
    "    df['rate_category'] = pd.cut(df.rate,bins=[df_rate_count['rate'].min()-1,-1,0,\\\n",
    "                                           df_rate_count['rate'].max()+1],labels=[-1,0,1])\n",
    "    return df['rate_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StrLower (series):\n",
    "    #Перевод в нижний регистр\n",
    "    col = series.str.lower()\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_tokenizer(s):\n",
    "    '''\n",
    "    Нормализация текста\n",
    "    '''\n",
    "    morph = pymorphy2.MorphAnalyzer()    \n",
    "    if type(s) == str:\n",
    "        t = s.split(' ')\n",
    "    else:\n",
    "        t = s\n",
    "    f = []\n",
    "    for j in t:\n",
    "        m = morph.parse(j.replace('.',''))\n",
    "        if len(m) != 0:\n",
    "            wrd = m[0]\n",
    "            if wrd.tag.POS not in ('NUMR','PREP','CONJ','PRCL','INTJ'):\n",
    "                f.append(wrd.normal_form)\n",
    "    return ' '.join(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddNewFeature_norm(df):\n",
    "    df['ru_words_text']=df['ru_words_text'].apply(lambda t: f_tokenizer(t))\n",
    "    return df['ru_words_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конвеер обработки данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csv_db = create_engine('sqlite:///dataset/csv_db.db', poolclass=NullPool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Добавлю логирование\n",
    "LOG_FILENAME = 'export_to_db.log'\n",
    "logging.basicConfig(filename='dataset/'+LOG_FILENAME,\\\n",
    "                    level=logging.ERROR,\\\n",
    "                    format='%(asctime)s: %(message)s',\\\n",
    "                    datefmt='%Y-%m-%d %H-%M-%S')\n",
    "logger = logging.getLogger('db')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 1000 #Сколько строк брать за раз\n",
    "text_column = 'text'\n",
    "tittle_column = 'tittle'\n",
    "parts_text_columns = ['sites_text', 'ru_words_text','en_words_text']\n",
    "iteration=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Подготавливаем данные и пишем в БД\n",
    "start = 193001 #Если нужно начать/продолжить с определенной строки, например с 60001\n",
    "for df in pd.read_csv('dataset/clear_dataset.csv', chunksize=chunksize, iterator=True,sep=',',\\\n",
    "                        encoding='utf-8', engine=\"python\", error_bad_lines=False):\n",
    "    df.index +=1\n",
    "    \n",
    "    if df.index[-1] <start:\n",
    "         pass\n",
    "    else:\n",
    "        logger.info('Итерация [{}]'.format(iteration))\n",
    "        logger.info('   Очистка от пустых данных')\n",
    "        df.drop(CleanDataFirstSatge(df),inplace=True)  \n",
    "    \n",
    "        logger.info('   Геренерация фич на основе времени')\n",
    "        df['date'],df['month'],df['day'],df['hour'],df['weekday'] = AddNewFeature_dates(df)  \n",
    "    \n",
    "        logger.info('   Генерация числовых и категориальных фич на основе текста')\n",
    "        for index, row in df.iterrows():\n",
    "            df.loc[index, 'sites_'+text_column],df.loc[index, 'sites_'+text_column+'_count'] = GetSites(row[text_column])\n",
    "            df.loc[index, 'ru_words_'+text_column],df.loc[index, 'ru_words_'+text_column+'_count'] = GetRuWords(row[text_column])\n",
    "            df.loc[index, 'en_words_'+text_column],df.loc[index, 'en_words_'+text_column+'_count'] = GetEnWords(row[text_column])\n",
    "            df.loc[index, 'punctuations_'+text_column+'_count'] = GetPunctuationsCount(row[text_column])\n",
    "            df.loc[index, 'emotions_'+text_column+'_count'] = GetEmotionsCount(row[text_column])\n",
    "            df.loc[index, text_column+'_words_count'],df.loc[index, text_column+'_clwords_count'] = GetAllWordsCount(row[text_column])\n",
    "            df.loc[index, tittle_column+'_words_count'],df.loc[index, tittle_column+'_clwords_count'] = GetAllWordsCount(row[tittle_column])\n",
    "    \n",
    "        logger.info('   Удаление статей без текста')\n",
    "        df.drop(df[df['text_words_count']==0].index,inplace=True)\n",
    "    \n",
    "        logger.info('   Фича категория текста')\n",
    "        df['text_category'] = AddNewFeature_text_category(df)\n",
    "    \n",
    "        logger.info('   Фича категория рейтинга')\n",
    "        df['rate_category'] = AddNewFeature_rate_category(df)\n",
    "        \n",
    "        logger.info('   Перевод текста в нижний регистр')\n",
    "        for col in parts_text_columns:\n",
    "            df[col] = StrLower(df[col])\n",
    "    \n",
    "        logger.info('   Нормализация текста')\n",
    "        df['ru_words_text'] = AddNewFeature_norm(df)    \n",
    "    \n",
    "        logger.info('   Запись в БД')\n",
    "        df.to_sql('habr', csv_db, if_exists='append')\n",
    "    \n",
    "        logger.info('   Всего обработано строк [{}]'.format(iteration*chunksize))\n",
    "    iteration+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.shutdown() #Убираю логирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all handlers associated with the root logger object.\n",
    "#for handler in logging.root.handlers[:]:\n",
    "#    logging.root.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В итоге обработка заняла ~12.5 часов. В БД находятся все статьи, оригинальные столбцы + новые, сгенерированные фичи, так же нормализованный текст из столбца text.\n",
    "\n",
    "Описание столбцов:\n",
    "Оригинальные:\n",
    "1. date - дата и время публикации статьи\n",
    "2. tittle - заголовок статьи\n",
    "3. text - текст статьи\n",
    "4. rate - рейтинг статьи\n",
    "Сгенерированные фичи:\n",
    "1. month - месяц публикации статьи\n",
    "2. day - число публикации статьи\n",
    "3. hour - час публикации статьи\n",
    "4. weekday - день недели публикации статьи (0-п,6-вс)\n",
    "5. sites_text - сайты, извлеченные из столбца text в формате \"cnews_ru\"\n",
    "6. sites_text_count - количество сайтов в столбце text\n",
    "7. ru_words_text - нормализованные слова на кириллице, извлеченные из text\n",
    "8. ru_words_text_count - количество слов в text (до нормализации)\n",
    "9. en_words_text - слова на латинице, извлеченные из text\n",
    "10. en_words_text_count - количество слов на латинице\n",
    "11. punctuations_text_count - количество пунктуаций в text\n",
    "12. emotions_text_count - количество знаков эмоций в text(!!!,???)\n",
    "13. text_words_count - количество всех текстовых объектов из text разделенных пробелом\n",
    "14. text_clwords_count - количество всех слов с заглавной буквы в text\n",
    "15. tittle_words_count - количество всех текстовых объектов из tittle разделенных пробелом\n",
    "16. tittle_clwords_count - количество всех слов с заглавной буквы в tittle\n",
    "17. rate_category - категория рейтинга статьи (отриц., нейтральный, положительн.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(\"SELECT * FROM habr\", csv_db, index_col='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ на основе 20000 статей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Текстовые данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно провести анализ на наличие мусорных статей, таких как состоящих только из цифр, повторящихся заголовков, пустых и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на статьи с повторяющимися заголовками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TC = df['tittle'].value_counts().rename('count').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ttop = np.array(df_TC[df_TC['count']>1]['index']) #отберем все повторяющиеся заголовки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_Ttop = df[df['tittle'].apply(lambda t: t in Ttop)].sort_values(by='tittle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_Ttop['tittle'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Ttop[df_Ttop['tittle']=='удаленное']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможно статьи которые содержат слова 'test','тест','удаленное','удалено','удалил','deleted','delete'\n",
    "стоит исключить из выборки, т.к. рейтинг у таких статей как правило отрицательный и может влиять на предсказание других статей содержащие такие слова."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же к фейковым статьям скорее всего можно отнести статьи, состоящие только из одного слова/символа или цифр.Так же есть статьи с пустым полем 'text'. Посмотрим на них."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_for_drop = df[((df['text'].apply(lambda t: len(t.strip().split()))<=1)\\\n",
    "  & (df['tittle'].apply(lambda t: len(t.strip().split()))<=1))\\\n",
    "  | (df['text'] == '   ')]\n",
    "df_for_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такие статьи решил удалить. Так же нужно сделать новые признаки по количеству слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df_for_drop.index,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>#new_feature</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddNewFeature_tittle_words_count():\n",
    "    df['tittle_words_count'] = df['tittle'].apply(lambda t: len(t.strip().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddNewFeature_text_words_count():\n",
    "    df['text_words_count'] = df['text'].apply(lambda t: len(t.strip().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AddNewFeature_tittle_words_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AddNewFeature_text_words_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['text_words_count']==0].index,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество статей в зависимости от количества слов в заголовке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df['tittle_words_count']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fsize = (10, 5)\n",
    "fig, ax = plt.subplots(figsize=fsize)\n",
    "df['tittle_words_count'].value_counts().plot(kind='barh');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большинство заголовков статей не превышает 10 слов. Больше всего статей с 3-6 словами в заголовке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Статьи в зависимости от количества слов в тексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsize = (15, 5)\n",
    "fig, ax = plt.subplots(figsize=fsize)\n",
    "sns.boxplot(x=df['text_words_count']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_words_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['text_words_count']>1000]['text'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "БОльшая часть статей не превышает 370 слов, хотя есть и статьи от 1000 слов, но их не много."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создам новый признак, который будет отвечать за размер статьи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>#new_feature</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddNewFeature_text_category():\n",
    "    twc25 = df['text_words_count'].describe()['25%']\n",
    "    twc50 = df['text_words_count'].describe()['50%']\n",
    "    twc75 = df['text_words_count'].describe()['75%']\n",
    "    twcmax = df['text_words_count'].describe()['max']\n",
    "    \n",
    "    df['text_category'] = pd.cut(df.text_words_count\\\n",
    "                             ,bins=[-1,0,twc25,twc50,twc75,twcmax]\\\n",
    "                             ,labels=['z','s','m','l','xl'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AddNewFeature_text_category()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_category'].value_counts().plot(kind='barh');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создам еще несколко фичей по тексту"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>#new_feature</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetCapitalLetterWordsCount (txt):\n",
    "    #Количество слов с заглавной буквы\n",
    "    clwords  = re.findall(r'[А-ЯA-Z]{1,}[а-яa-z]{1,}\\b',txt) \n",
    "    return len(clwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddNewFeature_text_clwords_count():\n",
    "    df['text_clwords_count'] = df['text'].\\\n",
    "                            apply(lambda t: GetCapitalLetterWordsCount(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AddNewFeature_text_clwords_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>#new_feature</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddNewFeature_tittle_words_count():\n",
    "    #Количество \"слов\" в tittle\n",
    "    df['tittle_words_count'] = df['tittle'].apply(lambda t: len(t.strip().\\\n",
    "                                                                split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddNewFeature_text_words_count():\n",
    "    #Количество \"слов\" в text\n",
    "    df['text_words_count'] = df['text'].apply(lambda t: len(t.strip().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AddNewFeature_tittle_words_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AddNewFeature_text_words_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переведем все слова в нижний регистр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StrLower (series):\n",
    "    col = series.str.lower()\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tittle'] = StrLower(df['tittle'])\n",
    "df['text'] = StrLower(df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рейтинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsize = (15, 5)\n",
    "fig, ax = plt.subplots(figsize=fsize)\n",
    "sns.violinplot(x=df['rate'], palette=\"Blues\");\n",
    "sns.boxplot(x=df['rate']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rate'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rate_count = df['rate'].value_counts().rename('count').reset_index()\n",
    "df_rate_count.rename({'index':'rate'}, axis='columns',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rate_count.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4_dims = (18.7, 8.27)\n",
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "sns.barplot(y='count',x='rate',data=df_rate_count[df_rate_count['count']>100]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Топ популярных рейтингов лежит в диапазоне от 0-6. Самый часто используемый рейтинг - 0, т.е. статья не оценивается?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на средний рейтинг статей в зависимости от количества слов в заголовке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsize = (12, 5)\n",
    "fig, ax = plt.subplots(figsize=fsize)    \n",
    "sns.barplot(x='tittle_words_count', y='rate_avg', \\\n",
    "            data=df.groupby('tittle_words_count')['rate'].mean().\\\n",
    "                                            rename('rate_avg').reset_index());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В среднем,у статей с количеством слов в заголовке мене 19 слов рейтинг положительный и не превышает 20.\n",
    "\n",
    "Максимальный положительный рейтинг у статей с количеством слов в заголовке 18-19. Но дальше резкое снижение рейтинга!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на рейтинг статей в зависимости от размера статьи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsize = (15, 5)\n",
    "fig, ax = plt.subplots(figsize=fsize)\n",
    "sns.boxplot(y='rate',x='text_category',data=df[(df['rate']<60)&\\\n",
    "                                              (df['rate']>-30)]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='text_category', y='rate_avg', \\\n",
    "            data=df.groupby('text_category')['rate'].mean().\\\n",
    "                                            rename('rate_avg').reset_index());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что чем больше статья, тем выше рейтинг."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделю новую фичу - категория рейтинга. Разделю все статьи на отрицательные, нейтральные и положительные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>#new_feature</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddNewFeature_rate_category ():\n",
    "    #Разделю рейтинги на 3 категории (отрицательный, нейтральный и положительный)\n",
    "    df['rate_category'] = pd.cut(df.rate,bins=[df_rate_count['rate'].min()-1,-1,0,\\\n",
    "                                           df_rate_count['rate'].max()+1],labels=[-1,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AddNewFeature_rate_category ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df['rate_category']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Статей с положительным рейтингом значительно больше, чем остальных.\n",
    "\n",
    "Средний рейтинг категории:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('rate_category')['rate'].mean().rename('rate_avg').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распределение по категориям рейтинга количества статей в зависимости от категории текста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataTC = df.groupby(['text_category'], observed=True)['rate_category'].\\\n",
    "         value_counts(normalize=True).rename('count').reset_index().\\\n",
    "         sort_values('text_category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='text_category', y='count', hue='rate_category',data=dataTC);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В основном тенденция роста количества статей по рейтингу, но в категории m больше всего статей с отрицательным рейтингом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataTWC(normalize):\n",
    "    dataTWC= df.groupby(['tittle_words_count'])['rate_category'].value_counts(normalize=normalize).\\\n",
    "    rename('count').reset_index()\n",
    "    fsize = (12, 5)\n",
    "    fig, ax = plt.subplots(figsize=fsize)   \n",
    "    sns.barplot(x='tittle_words_count', y='count', hue='rate_category',data=dataTWC);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTWC(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTWC(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интересно, что статьи с количеством слов 20-24 в заголовке всегда отрицательные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на рисунок зависимости количества слов в заголовке и рейтинга, диффиренцированных по размеру статьи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsize = (12, 6)\n",
    "fig, ax = plt.subplots(figsize=fsize)   \n",
    "plt.scatter(df['tittle_words_count'],df['rate'],\\\n",
    "           color=df['text_category'].map({'z':'blue','s':'orange',\\\n",
    "                                          'm':'green','l':'red','xl':'violet'}));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='text_words_count', y='tittle_words_count', data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['rate','text_words_count','tittle_words_count']].corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(df[['rate','text_words_count','tittle_words_count']].corr(method='pearson'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какой то явной корреляции не видно. Есть небольшая зависимость длины слов в статье с рейтингом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дата и время"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создам новые признаки, для этого приведем 'date' к формату даты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>#new_feature</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddNewFeature_dates():\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['month'] = df['date'].dt.month \n",
    "    df['day'] = df['date'].dt.day \n",
    "    df['hour'] = df['date'].dt.hour \n",
    "    df['weekday'] = df['date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AddNewFeature_dates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BarGroupByTwo (col1,col2,normalize:bool):    \n",
    "    data = df.groupby([col1])[col2].value_counts(normalize=normalize).rename('count').reset_index().sort_values(col1)\n",
    "    \n",
    "    a4_dims = (11.7, 8.27)\n",
    "    fig, ax = plt.subplots(figsize=a4_dims)    \n",
    "    sns.barplot(x=col1, y='count', hue=col2, data=data)\n",
    "    \n",
    "    normalize_label=''\n",
    "    if normalize:\n",
    "        normalize_label=' (%)'\n",
    "    plt.title('{} vs {}{}'.format(col2, col1,normalize_label))\n",
    "    \n",
    "    labels=[\"Отрицательный\",\"Нейтральный\", \"Положительный\"]\n",
    "    h, l = ax.get_legend_handles_labels()\n",
    "    ax.legend(h, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на зависимость рейтинга от времЕнных параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BarGroupByTwo('month','rate_category',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BarGroupByTwo('month','rate_category',True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Июль, август, сентябрь и октябрь, месяца, когда меньше всего выходят статьи. В эти месяца количество отрицательных отзывов больше чем в остальные, в процентном соотношении так примерно в 2 раза."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BarGroupByTwo('weekday','rate_category',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BarGroupByTwo('weekday','rate_category',True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В субботу и воскресенье (5 и 6) выходит меньше всего статей. При этом, если смотреть в процентном соотношении, то в эти дни больше отрицательных оценок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BarGroupByTwo('day','rate_category',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BarGroupByTwo('day','rate_category',True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество статей публикуется волнообразно, видимо это связано с началом и концом недели. Так же в последнии дни месяца заметно снижение в количестве публикаций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BarGroupByTwo('hour','rate_category',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BarGroupByTwo('hour','rate_category',True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пик публикаций утро и день. С 12-14 часов заметно больше ставят отрицательных оценок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df[['rate', 'month', 'day', 'hour', 'weekday']].corr(method='pearson'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сильной корреляции нет. Небольшая корреляция с рейтингом у дней недели, чисел и часов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearRegression, Lasso, Ridge, LassoCV, RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso, Ridge, LassoCV, RidgeCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression = LinearRegression()\n",
    "lasso = Lasso(random_state=1,alpha=1)\n",
    "ridge = Ridge(random_state=1, alpha=1)\n",
    "lasso_cv = LassoCV(random_state=1)\n",
    "ridge_cv = RidgeCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models = [linear_regression,ridge,ridge_cv,lasso,lasso_cv]\n",
    "models = [linear_regression,ridge,lasso]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metricsModel (y_train,y_train_pred,y_test,y_test_pred):\n",
    "    '''\n",
    "    расчет метрик\n",
    "    '''    \n",
    "    metrics = []\n",
    "    MSETrain = mean_squared_error(y_train, y_train_pred)\n",
    "    MSETest = mean_squared_error(y_test, y_test_pred)\n",
    "    metrics.append(MSETrain)\n",
    "    metrics.append(MSETest)\n",
    "    metrics.append(np.sqrt(MSETrain))\n",
    "    metrics.append(np.sqrt(MSETest))\n",
    "    metrics.append(r2_score(y_train, y_train_pred))\n",
    "    metrics.append(r2_score(y_test, y_test_pred))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predGroundTruth (predicted_train,predicted_test,y_train,y_test,row=20):\n",
    "    '''\n",
    "    сравнение предсказаных рейтингов с реальными на трейне и тесте\n",
    "    '''  \n",
    "    predictions_ground_truth_df = pd.DataFrame(list(zip(predicted_train,\\\n",
    "                                                        y_train,predicted_test, y_test)))\n",
    "    predictions_ground_truth_df.columns = ['PredictionRateTrain', 'RrealRateTrain',\\\n",
    "                                           'PredictionRateTest', 'RrealRateTest']\n",
    "    print(predictions_ground_truth_df.head(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateTableInfoResult ():\n",
    "    '''\n",
    "    сюда буду сохранять результаты обучения моделей для анализа\n",
    "    '''\n",
    "    \n",
    "    modelResults_df=pd.DataFrame(columns=['Comment','Model','MSETrain','MSETest',\\\n",
    "          'sMSETrain','sMSETest','R2Train','R2Test','TrainShape','TestShape'])\n",
    "    return modelResults_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelResults_df = CreateTableInfoResult()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_min = df['rate'].min()+1e-8\n",
    "def LogConvert(rlog,rmin):\n",
    "    '''\n",
    "    конвертация рейтинга обратно из нартурального логарифма\n",
    "    '''\n",
    "    try:\n",
    "        rate = math.exp(rlog) + rmin\n",
    "    except OverflowError:\n",
    "        rate = float('inf')    \n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogNdArrayConvert(ndarray):\n",
    "    ndarray_c = np.array([LogConvert(r,rate_min) for r in ndarray])\n",
    "    return ndarray_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelFitPredictAndMettrics(mr_df,comment,model,X_train,y_train,X_test,y_test\\\n",
    "                              ,groundtruth=False,metrics=True,logconvert=False,row=20):\n",
    "    '''\n",
    "    обучение модели\n",
    "    '''\n",
    "    print(model)    \n",
    "    model.fit(X_train, y_train)\n",
    "    predicted_train = model.predict(X_train)\n",
    "    predicted_test = model.predict(X_test)\n",
    "    mtxt = ''\n",
    "    \n",
    "    if (logconvert):\n",
    "        predicted_train = LogNdArrayConvert(predicted_train)\n",
    "        predicted_test = LogNdArrayConvert(predicted_test)\n",
    "        y_train = LogNdArrayConvert(y_train)\n",
    "        y_test = LogNdArrayConvert(y_test)\n",
    "    \n",
    "    if(groundtruth):\n",
    "         predGroundTruth(predicted_train,predicted_test,y_train,y_test,row)\n",
    "    \n",
    "    mm = metricsModel(y_train,predicted_train,y_test,predicted_test)\n",
    "    if(metrics):       \n",
    "           print('\\nMSE train:{:.3f}, test: {:.3f}'.format(mm[0],mm[1]))  \n",
    "           print('sqrt MSE train:{:.3f}, test: {:.3f}'.format(mm[2],mm[3]))\n",
    "           print('R^2 train: {:.3f}, test: {:.3f}'.format(mm[4],mm[5]))\n",
    "        \n",
    "    mr_df = mr_df.append({'Comment':comment,'Model':model,'MSETrain':mm[0],\\\n",
    "                         'MSETest':mm[1],'sMSETrain':mm[2],'sMSETest':mm[3],\\\n",
    "                         'R2Train':mm[4],'R2Test':mm[5],\\\n",
    "                         'TrainShape':X_train.shape,'TestShape':X_test.shape},\\\n",
    "                         ignore_index=True)\n",
    "    return mr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторизация текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбираюсь, что это такое и как работает на основе столбца tittle. \n",
    "Посмотрю на модели в лоб, данные как есть, а потом буду улучшать их и добавлять фичи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к. у нас есть временной параметр, и мы хотим прогнозировать будущее, то нужно разбирть выборку на части отсортированную по времени. Учимся на прошлом, прогнозируем будущее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainTestSplitByDateTime (coef,df=df,y_col='rate'):\n",
    "    df.sort_values(by='date',ascending=True,inplace=True)\n",
    "    df.reset_index(drop=True,inplace=True);\n",
    "    \n",
    "    rate_cols = [col for col in df.columns if 'rate' in col]\n",
    "    X=df.drop(rate_cols,axis=1)\n",
    "    y=df[y_col]\n",
    "    train_part_size = int(coef * X.shape[0])\n",
    "    \n",
    "    X_train, X_test = X.iloc[:train_part_size, :],X.iloc[train_part_size:, :]\n",
    "    y_train, y_test = y.iloc[:train_part_size],y.iloc[train_part_size:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = TrainTestSplitByDateTime (.8,df,'rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape train:{},test:{}'.format(X_train.shape,X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизация и векторизация текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnOne = 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_curr = X_train[columnOne]\n",
    "X_test_curr = X_test[columnOne]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(max_df=1.0, min_df=1,ngram_range=(1, 1)) #токенезированный мешок слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cvect_train = count_vect.fit_transform(X_train_curr) \n",
    "X_cvect_test = count_vect.transform(X_test_curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#обучаемся на мешке слов и кодируем текст в вектора\n",
    "X_cvect_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cvect_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очень длинный вектор, много слов! Нужно чистить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(count_vect.get_feature_names()) #Мешок слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# словарь с идентификаторами расположения слов в словаре\n",
    "print(count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#закодированный (векторизированный) текст (матрица). Количество вхождений слова в словарь\n",
    "#X_cvect_train.toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# количественное вхождение каждого слова:\n",
    "matrix_freq = np.asarray(X_cvect_train.sum(axis=0)).ravel()\n",
    "final_matrix = np.array([np.array(count_vect.get_feature_names()), matrix_freq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цифры и китайские символы, нужно избавляться от них."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Векторизация с вычисление частот слов (Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_train = tfidf_transformer.fit_transform(X_cvect_train)\n",
    "X_tfidf_test = tfidf_transformer.fit_transform(X_cvect_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Tfidf матрица векторов с весами\n",
    "#X_tfidf_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#топ весов слов\n",
    "rr = dict(zip(count_vect.get_feature_names(), tfidf_transformer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "token_weight = pd.DataFrame.from_dict(rr, orient='index').reset_index()\n",
    "token_weight.columns=('token','weight')\n",
    "token_weight = token_weight.sort_values(by='weight', ascending=False)\n",
    "token_weight.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_weight.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Низкий вес у часто встречающихся слов, как раз тут видно, что это предлоги и частицы, от них тоже нужно избавиться в словаре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.isnan(X_tfidf_train.toarray().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Односторонний хэш слов для преобразования их в целые числа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_vect = HashingVectorizer(n_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hashvect_train = hash_vect.fit_transform(X_train_curr)\n",
    "X_hashvect_test = hash_vect.transform(X_test_curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# summarize encoded vector\n",
    "print(X_hashvect_train.shape)\n",
    "print(X_hashvect_train.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(X_hashvect_test.shape)\n",
    "print(X_hashvect_test.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При большом количестве данных словарь получается очень большой, очень много мусорных слов. При больших выборках (>40000) результирующая матрица на столько большая, что не хватает памяти. Необходимо обработать текст."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполню простую нормализация текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_tokenizer(s):\n",
    "    morph = pymorphy2.MorphAnalyzer()    \n",
    "    if type(s) == str:\n",
    "        t = s.split(' ')\n",
    "    else:\n",
    "        t = s\n",
    "    f = []\n",
    "    for j in t:\n",
    "        m = morph.parse(j.replace('.',''))\n",
    "        if len(m) != 0:\n",
    "            wrd = m[0]\n",
    "            if wrd.tag.POS not in ('NUMR','PREP','CONJ','PRCL','INTJ'):\n",
    "                f.append(wrd.normal_form)\n",
    "    return ' '.join(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>#new_feature</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddNewFeature_norm():\n",
    "    df[columnOne+'_norm']=df[columnOne].apply(lambda t: f_tokenizer(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AddNewFeature_norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[[columnOne,columnOne+'_norm']].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделю слова на кирилице и латинице а так же сайты в отдельные словари"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создам еще несколько фичей на основе текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>#new_feature</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSites (txt):  \n",
    "    sites = re.findall(r'[\\w]+\\.{1,1}[^\\.\\d^\\s]\\w{1,3}\\b',str(txt)) #сайты\n",
    "    return (' '.join(sites)).replace('.','_'),len(sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetEnWords (txt):  \n",
    "    en_words = re.findall(r'\\b[a-z]{3,}\\b',re.sub(r'[a-z]{2,}\\.+[a-z]*\\b','',str(txt))) #слова на латинице от 3х букв    \n",
    "    return ' '.join(en_words),len(en_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetRuWords (txt):  \n",
    "    ru_words = re.findall(r'\\b[а-я]{4,}\\b',re.sub(r'[а-я]{2,}\\.+[а-я]*\\b','',str(txt))) #слова на кириллице от 4х букв\n",
    "    return ' '.join(ru_words),len(ru_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetPunctuationsCount (txt):  \n",
    "    punctuations  = re.findall(r'[^\\w\\s]',txt) #Знаки препинания\n",
    "    return len(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetEmotionsCount (txt):  \n",
    "    emotions  = re.findall(r'[\\?]{2,}|[\\!]{2,}',txt) #Знаки ??? , !!!\n",
    "    return len(emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddNewFeature_split_txt(column):\n",
    "    for index, row in df.iterrows():\n",
    "        df.loc[index, 'sites_'+column],df.loc[index, 'sites_'+column+'_count'] = GetSites(row[column])\n",
    "        df.loc[index, 'ru_words_'+column],df.loc[index, 'ru_words_'+column+'_count'] = GetRuWords(row[column+'_norm'])\n",
    "        df.loc[index, 'en_words_'+column],df.loc[index, 'en_words_'+column+'_count'] = GetEnWords(row[column])\n",
    "        df.loc[index, 'punctuations_'+column+'_count'] = GetPunctuationsCount(row[column])\n",
    "        df.loc[index, 'emotions_'+column+'_count'] = GetEmotionsCount(row[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AddNewFeature_split_txt(columnOne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>#new_feature</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заменю буквенные категории на числовые"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddNewFeature_text_category_int():\n",
    "    df['text_category_int'] = df['text_category'].map({'z':0,'s':1,'m':2,'l':3,'xl':4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AddNewFeature_text_category_int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на графики:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.heatmap(df[['rate', 'sites_text_count', 'ru_words_text_count', 'en_words_text_count',\\\n",
    "                'punctuations_text_count','emotions_text_count','text_category_int',\\\n",
    "               'tittle_words_count','text_words_count']].corr(method='pearson'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какой то сильной корреляции фич с рейтингом нет. Некоторые фичи хорошо коррелируют между собой, это и понятно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьму натуральные логарифмы от всех числовых признаков, кроме категориальных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rate_log'] = np.log(df['rate'] - df['rate'].min()+1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rate_cube_r'] = df['rate'].apply(lambda r: int(r)**(1/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_words_count_log'] = np.log(df['text_words_count'])\n",
    "df['tittle_words_count_log'] = np.log(df['tittle_words_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sites_text_count_log'] = np.log(df['sites_text_count'])\n",
    "df['ru_words_text_count_log'] = np.log(df['ru_words_text_count'])\n",
    "df['en_words_text_count_log'] = np.log(df['en_words_text_count'])\n",
    "df['punctuations_text_count_log'] = np.log(df['punctuations_text_count'])\n",
    "df['emotions_text_count_log'] = np.log(df['emotions_text_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сформирую новые словари и новые матрицы векторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формирую 3 словаря - ru,eng,sites, так же в этом случае уйдут все цифры, даты и прочие малоинформативные артифакты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же уберу слова, которые очень редко и очень часто встречаются"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features=1000 #n_features для HashingVectorizer\n",
    "cv_max_df=0.99 #Параметры для CountVectorizer default 1.0\n",
    "cv_min_df=0.005 #Параметры для CountVectorizer default 1\n",
    "cv_ngram_range=(1, 1) #Параметры для CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имя файла для сохранения таблицы с метриками в csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileNameFromExport = 'ex_habr_'+str(rowCountFromCSV)+'_'+str(n_features)+'_'+\\\n",
    "                      str(cv_max_df)+'_'+str(cv_min_df)+'_'+\\\n",
    "                      str(cv_ngram_range)+'_'+f\"{datetime.datetime.now():%Y-%m-%d %H-%M-%S}\"+'.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(max_df=cv_max_df, min_df=cv_min_df,\\\n",
    "                             ngram_range=cv_ngram_range)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "hash_vect = HashingVectorizer(n_features=n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = TrainTestSplitByDateTime (.8,df,'rate')\n",
    "X_train, X_test, y_train_l, y_test_l = TrainTestSplitByDateTime (.8,df,'rate_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = [[y_train,y_test,False],[y_train_l,y_test_l,True]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape train:{},test:{}'.format(X_train.shape,X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train_curr = X_train['ru_words_'+columnOne]\n",
    "X_test_curr = X_test['ru_words_'+columnOne]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cvect_ru_words_train = count_vect.fit_transform(X_train_curr) \n",
    "X_tfidf_ru_words_train = tfidf_transformer.fit_transform(X_cvect_ru_words_train)\n",
    "X_cvect_ru_words_test = count_vect.transform(X_test_curr) \n",
    "X_tfidf_ru_words_test = tfidf_transformer.transform(X_cvect_ru_words_test)\n",
    "X_hashvect_ru_words_train = hash_vect.fit_transform(X_train_curr)\n",
    "X_hashvect_ru_words_test = hash_vect.transform(X_test_curr)\n",
    "\n",
    "rr_ru = dict(zip(count_vect.get_feature_names(), tfidf_transformer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_cvect_ru_words_train.shape,X_tfidf_ru_words_train.shape,X_hashvect_ru_words_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_curr = X_train['en_words_'+columnOne]\n",
    "X_test_curr = X_test['en_words_'+columnOne]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cvect_en_words_train = count_vect.fit_transform(X_train_curr) \n",
    "X_tfidf_en_words_train = tfidf_transformer.fit_transform(X_cvect_en_words_train)\n",
    "X_cvect_en_words_test = count_vect.transform(X_test_curr) \n",
    "X_tfidf_en_words_test = tfidf_transformer.transform(X_cvect_en_words_test)\n",
    "X_hashvect_en_words_train = hash_vect.fit_transform(X_train_curr)\n",
    "X_hashvect_en_words_test = hash_vect.transform(X_test_curr)\n",
    "\n",
    "rr_en = dict(zip(count_vect.get_feature_names(), tfidf_transformer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Посмотрим на топ веса 2х словарей\n",
    "def token_weight_df (dic, prefix):    \n",
    "    token_weight = pd.DataFrame.from_dict(dic, orient='index').reset_index()\n",
    "    token_weight.columns=(prefix,'weight_'+prefix)\n",
    "    token_weight = token_weight.sort_values(by='weight_'+prefix, ascending=False)\n",
    "    return token_weight\n",
    "\n",
    "rr_2_voc = pd.concat([token_weight_df(rr_ru,'ru').reset_index(drop=True),\\\n",
    "                      token_weight_df(rr_en,'en').reset_index(drop=True),\\\n",
    "                     ],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_2_voc.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rr_2_voc.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сайты возьму с другими параметрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_max_df=1.0 #Параметры для CountVectorizer default 1.0\n",
    "cv_min_df=2 #Параметры для CountVectorizer default 1\n",
    "cv_ngram_range=(1, 1) #Параметры для CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(max_df=cv_max_df, min_df=cv_min_df,\\\n",
    "                             ngram_range=cv_ngram_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_curr = X_train['sites_'+columnOne]\n",
    "X_test_curr = X_test['sites_'+columnOne]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cvect_sites_train = count_vect.fit_transform(X_train_curr) \n",
    "X_tfidf_sites_train = tfidf_transformer.fit_transform(X_cvect_sites_train)\n",
    "X_cvect_sites_test = count_vect.transform(X_test_curr) \n",
    "X_tfidf_sites_test = tfidf_transformer.transform(X_cvect_sites_test)\n",
    "X_hashvect_sites_train = hash_vect.fit_transform(X_train_curr)\n",
    "X_hashvect_sites_test = hash_vect.transform(X_test_curr)\n",
    "\n",
    "rr_sites = dict(zip(count_vect.get_feature_names(), tfidf_transformer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_cvect_sites_train.shape,X_tfidf_sites_train.shape,X_hashvect_sites_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Посмотрим на топ веса\n",
    "rr_1_voc = token_weight_df(rr_sites,'sites').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_1_voc.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_1_voc.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформирую общую матрицу по 3м словарям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_cvect_3voc_train = hstack((X_cvect_sites_train, X_cvect_ru_words_train,\\\n",
    "                              X_cvect_en_words_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cvect_3voc_test = hstack((X_cvect_sites_test, X_cvect_ru_words_test,\\\n",
    "                              X_cvect_en_words_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_3voc_train = hstack((X_tfidf_sites_train, X_tfidf_ru_words_train,\\\n",
    "                              X_tfidf_en_words_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_3voc_test = hstack((X_tfidf_sites_test, X_tfidf_ru_words_test,\\\n",
    "                              X_tfidf_en_words_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hashvect_3voc_train = hstack((X_hashvect_sites_train, X_hashvect_ru_words_train,\\\n",
    "                              X_hashvect_en_words_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hashvect_3voc_test = hstack((X_hashvect_sites_test, X_hashvect_ru_words_test,\\\n",
    "                              X_hashvect_en_words_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_cvect_3voc_train.shape)\n",
    "print(X_cvect_3voc_test.shape)\n",
    "print(X_tfidf_3voc_train.shape)\n",
    "print(X_tfidf_3voc_test.shape)\n",
    "print(X_hashvect_3voc_train.shape)\n",
    "print(X_hashvect_3voc_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьму для обучения 5000 строк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_row = 5000\n",
    "X_cvect_3voc_train = X_cvect_3voc_train.tocsr()[:c_row].todense()\n",
    "X_tfidf_3voc_train = X_tfidf_3voc_train.tocsr()[:c_row].todense()\n",
    "X_hashvect_3voc_train = X_hashvect_3voc_train.tocsr()[:c_row].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_cvect_ru_words_train = X_cvect_ru_words_train.tocsr()[:c_row].todense()\n",
    "X_tfidf_ru_words_train = X_tfidf_ru_words_train.tocsr()[:c_row].todense()\n",
    "X_hashvect_ru_words_train = X_hashvect_ru_words_train.tocsr()[:c_row].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train[0:c_row]\n",
    "y_train_l = y_train_l[0:c_row]\n",
    "ys = [[y_train,y_test,False],[y_train_l,y_test_l,True]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение только на основе столбца 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelFit_cvect_ru_words_n(m,modelResults_df,X_cvect_ru_words_train,y_train,X_cvect_ru_words_test,y_test,ratelog=False):\n",
    "    comment=columnOne+ ' + Очистка + Нормализация + только ru_words cvect + rate_log:'+str(ratelog)\n",
    "    modelResults_df = ModelFitPredictAndMettrics(modelResults_df,comment,m,\\\n",
    "                      X_cvect_ru_words_train,y_train,X_cvect_ru_words_test,y_test,False,False,ratelog)\n",
    "    return modelResults_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelFit_tfidf_ru_words_n(m,modelResults_df,X_tfidf_ru_words_train,y_train,X_tfidf_ru_words_test,y_test,ratelog=False):\n",
    "    comment=columnOne+ ' + Очистка + Нормализация + только ru_words tfidf + rate_log:'+str(ratelog)\n",
    "    modelResults_df = ModelFitPredictAndMettrics(modelResults_df,comment,m,\\\n",
    "                      X_tfidf_ru_words_train,y_train,X_tfidf_ru_words_test,y_test,False,False,ratelog)\n",
    "    return modelResults_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelFit_hashvect_ru_words_n(m,modelResults_df,X_hashvect_ru_words_train,y_train,X_hashvect_ru_words_test,y_test,ratelog=False):\n",
    "    comment=columnOne+ ' + Очистка + Нормализация + только ru_words hashvect + rate_log:'+str(ratelog)\n",
    "    modelResults_df = ModelFitPredictAndMettrics(modelResults_df,comment,m,\\\n",
    "                      X_hashvect_ru_words_train,y_train,X_hashvect_ru_words_test,y_test,False,False,ratelog)\n",
    "    return modelResults_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelFit_cvect_3voc(m,modelResults_df,X_cvect_3voc_train,y_train,X_cvect_3voc_test,y_test,ratelog=False):\n",
    "    comment=columnOne+ ' + Очистка + Нормализация + разбиение на 3 словаря cvect + rate_log:'+str(ratelog)\n",
    "    modelResults_df = ModelFitPredictAndMettrics(modelResults_df,comment,m,\\\n",
    "                      X_cvect_3voc_train,y_train,X_cvect_3voc_test,y_test,False,False,ratelog)\n",
    "    return modelResults_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelFit_tfidf_3voc(m,modelResults_df,X_tfidf_3voc_train,y_train,X_tfidf_3voc_test,y_test,ratelog=False):\n",
    "    comment=columnOne+ ' + Очистка + Нормализация + разбиение на 3 словаря tfidf + rate_log:'+str(ratelog)\n",
    "    modelResults_df = ModelFitPredictAndMettrics(modelResults_df,comment,m,\\\n",
    "                      X_tfidf_3voc_train,y_train,X_tfidf_3voc_test,y_test,False,False,ratelog)\n",
    "    return modelResults_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelFit_hashvect_3voc(m,modelResults_df,X_hashvect_3voc_train,y_train,X_hashvecte_3voc_test,y_test,ratelog=False):\n",
    "    comment=columnOne+ ' + Очистка + Нормализация + разбиение на 3 словаря hashvect + rate_log:'+str(ratelog)\n",
    "    modelResults_df = ModelFitPredictAndMettrics(modelResults_df,comment,m,\\\n",
    "                      X_hashvect_3voc_train,y_train,X_hashvect_3voc_test,y_test,False,False,ratelog)\n",
    "    return modelResults_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y  in ys:\n",
    "    y_train=y[0]\n",
    "    y_test=y[1]\n",
    "    ratelogFlag=y[2]\n",
    "    for m in models:\n",
    "        modelResults_df = ModelFit_cvect_ru_words_n(m,modelResults_df,X_cvect_ru_words_train,y_train,X_cvect_ru_words_test,y_test,ratelogFlag)\n",
    "        modelResults_df = ModelFit_tfidf_ru_words_n(m,modelResults_df,X_tfidf_ru_words_train,y_train,X_tfidf_ru_words_test,y_test,ratelogFlag)\n",
    "        modelResults_df = ModelFit_hashvect_ru_words_n(m,modelResults_df,X_hashvect_ru_words_train,y_train,X_hashvect_ru_words_test,y_test,ratelogFlag)\n",
    "        modelResults_df = ModelFit_cvect_3voc(m,modelResults_df,X_cvect_3voc_train,y_train,X_cvect_3voc_test,y_test,ratelogFlag)\n",
    "        modelResults_df = ModelFit_tfidf_3voc(m,modelResults_df,X_tfidf_3voc_train,y_train,X_tfidf_3voc_test,y_test,ratelogFlag)\n",
    "        modelResults_df = ModelFit_hashvect_3voc(m,modelResults_df,X_hashvect_3voc_train,y_train,X_hashvect_3voc_test,y_test,ratelogFlag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение только на основе доп. фич"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = TrainTestSplitByDateTime (.8,df,'rate')\n",
    "X_train, X_test, y_train_l, y_test_l = TrainTestSplitByDateTime (.8,df,'rate_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = [[y_train,y_test,False],[y_train_l,y_test_l,True]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsAddFeatures = ['tittle_words_count', 'text_words_count',\\\n",
    "                    'text_category_int','sites_'+columnOne+'_count', \\\n",
    "            'ru_words_'+columnOne+'_count', 'en_words_'+columnOne+'_count',\\\n",
    "            'punctuations_'+columnOne+'_count','emotions_'+columnOne+'_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_choice_col = X_train[columnsAddFeatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_choice_col = X_test[columnsAddFeatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_choice_col_m = sparse.csr_matrix(X_train_choice_col)\n",
    "X_test_choice_col_m = sparse.csr_matrix(X_test_choice_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelFit_only_choice_col(m,modelResults_df,X_train_choice_col_m,y_train,X_test_choice_col_m,y_test,ratelog=False):\n",
    "    comment=columnOne + ' + только доп.фичи + rate_log:'+str(ratelog)+' + '+', '.join(columnsAddFeatures)\n",
    "    modelResults_df = ModelFitPredictAndMettrics(modelResults_df,comment,m,\\\n",
    "                      X_train_choice_col_m,y_train,X_test_choice_col_m,y_test,False,False,ratelog)\n",
    "    return modelResults_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же выполню шкалирование и посмотрю на результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# масштабируем данные\n",
    "X_train_choice_col_m_scaled = scaler.fit_transform(X_train_choice_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_choice_col_m_scaled = scaler.transform(X_test_choice_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelFit_only_choice_col_scaled(m,modelResults_df,X_train_choice_col_m_scaled,y_train,X_test_choice_col_m_scaled,y_test,ratelog=False):\n",
    "    comment=columnOne + ' + только доп.фичи + масштабирование ss + rate_log:'+str(ratelog)+' + ' +', '.join(columnsAddFeatures)\n",
    "    modelResults_df = ModelFitPredictAndMettrics(modelResults_df,comment,m,\\\n",
    "                      X_train_choice_col_m_scaled,y_train,X_test_choice_col_m_scaled,y_test,False,False,ratelog)\n",
    "    return modelResults_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for y  in ys:\n",
    "    y_train=y[0]\n",
    "    y_test=y[1]\n",
    "    ratelogFlag=y[2]\n",
    "    for m in models:\n",
    "        modelResults_df = ModelFit_only_choice_col(m,modelResults_df,\\\n",
    "                                                      X_train_choice_col_m,y_train,X_test_choice_col_m,y_test,ratelogFlag)\n",
    "        modelResults_df = ModelFit_only_choice_col_scaled(m,modelResults_df,\\\n",
    "                                        X_train_choice_col_m_scaled,y_train,X_test_choice_col_m_scaled,y_test,ratelogFlag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединю доп. фичи и векторайзеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_exp_m (X,vectorizer,dropindex=True):\n",
    "    if (dropindex):\n",
    "        exp = pd.concat([X.reset_index(drop=True),\\\n",
    "                           pd.DataFrame(vectorizer.toarray()).reset_index(drop=True)],axis=1)\n",
    "        exp[columnsAddFeatures] = exp[columnsAddFeatures].astype('float64')\n",
    "    else:\n",
    "        exp = pd.concat([X,\\\n",
    "                           pd.DataFrame(vectorizer.toarray()).reset_index(drop=True)],axis=1)   \n",
    "    exp.columns = range(exp.shape[1])\n",
    "    exp_m = sparse.csr_matrix(exp)\n",
    "    return exp_m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_choice_col = X_train_choice_col[:c_row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train[0:c_row]\n",
    "y_train_l = y_train_l[0:c_row]\n",
    "ys = [[y_train,y_test,False],[y_train_l,y_test_l,True]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_cvect_exp_m_train = Create_exp_m (X_train_choice_col,csr_matrix(X_cvect_3voc_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cvect_exp_m_test = Create_exp_m (X_test_choice_col,X_cvect_3voc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_exp_m_train = Create_exp_m (X_train_choice_col,csr_matrix(X_tfidf_3voc_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_exp_m_test = Create_exp_m (X_test_choice_col,X_tfidf_3voc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_hashvect_exp_m_train = Create_exp_m (X_train_choice_col,csr_matrix(X_hashvect_3voc_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hashvect_exp_m_test = Create_exp_m (X_test_choice_col,X_hashvect_3voc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_cvect_exp_m_train.shape)\n",
    "print(X_tfidf_exp_m_train.shape)\n",
    "print(X_hashvect_exp_m_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelFit_cvect_exp_m(m,modelResults_df,X_cvect_exp_m_train,y_train,X_cvect_exp_m_test,y_test,ratelog=False):\n",
    "    comment=columnOne+ ' + Очистка + Нормализация + разбиение на 3 словаря cvect + rate_log:'+str(ratelog)+' + доп.фичи + '+ ', '.join(columnsAddFeatures)\n",
    "    modelResults_df = ModelFitPredictAndMettrics(modelResults_df,comment,m,\\\n",
    "                      X_cvect_exp_m_train,y_train,X_cvect_exp_m_test,y_test,False,False,ratelog)\n",
    "    return modelResults_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelFit_tfidf_exp_m(m,modelResults_df,X_tfidf_exp_m_train,y_train,X_tfidf_exp_m_test,y_test,ratelog=False):\n",
    "    comment=columnOne+ ' + Очистка + Нормализация + разбиение на 3 словаря tfidf + rate_log:'+str(ratelog)+' + доп.фичи + '+ ', '.join(columnsAddFeatures)\n",
    "    modelResults_df = ModelFitPredictAndMettrics(modelResults_df,comment,m,\\\n",
    "                      X_tfidf_exp_m_train,y_train,X_tfidf_exp_m_test,y_test,False,False,ratelog)\n",
    "    return modelResults_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelFit_hashvect_exp_m(m,modelResults_df,X_hashvect_exp_m_train,y_train,X_hashvect_exp_m_test,y_test,ratelog=False):\n",
    "    comment=columnOne+ ' + Очистка + Нормализация + разбиение на 3 словаря hashvect + rate_log:'+str(ratelog)+' + доп.фичи + '+ ', '.join(columnsAddFeatures)\n",
    "    modelResults_df = ModelFitPredictAndMettrics(modelResults_df,comment,m,\\\n",
    "                      X_hashvect_exp_m_train,y_train,X_hashvect_exp_m_test,y_test,False,False,ratelog)\n",
    "    return modelResults_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y  in ys:\n",
    "    y_train=y[0]\n",
    "    y_test=y[1]\n",
    "    ratelogFlag=y[2]\n",
    "    for m in models:\n",
    "        modelResults_df = ModelFit_cvect_exp_m(m,modelResults_df,X_cvect_exp_m_train,y_train,X_cvect_exp_m_test,y_test,ratelogFlag)\n",
    "        modelResults_df = ModelFit_tfidf_exp_m(m,modelResults_df,X_tfidf_exp_m_train,y_train,X_tfidf_exp_m_test,y_test,ratelogFlag)\n",
    "        modelResults_df = ModelFit_hashvect_exp_m(m,modelResults_df,X_hashvect_exp_m_train,y_train,X_hashvect_exp_m_test,y_test,ratelogFlag)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тоже самое, но с масштабированными данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_choice_col_m_scaled = X_train_choice_col_m_scaled[:c_row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_cvect_exp_m_scaled_train = Create_exp_m (pd.DataFrame(X_train_choice_col_m_scaled),csr_matrix(X_cvect_3voc_train),False)\n",
    "X_cvect_exp_m_scaled_test = Create_exp_m (pd.DataFrame(X_test_choice_col_m_scaled),csr_matrix(X_cvect_3voc_test),False)\n",
    "X_tfidf_exp_m_scaled_train = Create_exp_m (pd.DataFrame(X_train_choice_col_m_scaled),csr_matrix(X_tfidf_3voc_train),False)\n",
    "X_tfidf_exp_m_scaled_test = Create_exp_m (pd.DataFrame(X_test_choice_col_m_scaled),csr_matrix(X_tfidf_3voc_test),False)\n",
    "X_hashvect_exp_m_scaled_train = Create_exp_m (pd.DataFrame(X_train_choice_col_m_scaled),csr_matrix(X_hashvect_3voc_train),False)\n",
    "X_hashvect_exp_m_scaled_test = Create_exp_m (pd.DataFrame(X_test_choice_col_m_scaled),csr_matrix(X_hashvect_3voc_test),False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelFit_cvect_exp_m_scaled(m,modelResults_df,X_cvect_exp_m_scaled_train,y_train,X_cvect_exp_m_scaled_test,y_test,ratelog=False):\n",
    "    comment=columnOne+ ' + Очистка + Нормализация + разбиение на 3 словаря cvect + rate_log:'+str(ratelog)+' + доп.фичи + масштабирование ss + '+ ', '.join(columnsAddFeatures)\n",
    "    modelResults_df = ModelFitPredictAndMettrics(modelResults_df,comment,m,\\\n",
    "                      X_cvect_exp_m_scaled_train,y_train,X_cvect_exp_m_scaled_test,y_test,False,False,ratelog)\n",
    "    return modelResults_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelFit_tfidf_exp_m_scaled(m,modelResults_df,X_tfidf_exp_m_scaled_train,y_train,X_tfidf_exp_m_scaled_test,y_test,ratelog=False):\n",
    "    comment=columnOne+ ' + Очистка + Нормализация + разбиение на 3 словаря tfidf + rate_log:'+str(ratelog)+' + доп.фичи + масштабирование ss + '+ ', '.join(columnsAddFeatures)\n",
    "    modelResults_df = ModelFitPredictAndMettrics(modelResults_df,comment,m,\\\n",
    "                      X_tfidf_exp_m_scaled_train,y_train,X_tfidf_exp_m_scaled_test,y_test,False,False,ratelog)\n",
    "    return modelResults_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelFit_hashvect_exp_m_scaled(m,modelResults_df,X_hashvect_exp_m_scaled_train,y_train,X_hashvect_exp_m_scaled_test,y_test,ratelog=False):\n",
    "    comment=columnOne+ ' + Очистка + Нормализация + разбиение на 3 словаря hashvect + rate_log:'+str(ratelog)+' + доп.фичи + масштабирование ss + '+ ', '.join(columnsAddFeatures)\n",
    "    modelResults_df = ModelFitPredictAndMettrics(modelResults_df,comment,m,\\\n",
    "                      X_hashvect_exp_m_scaled_train,y_train,X_hashvect_exp_m_scaled_test,y_test,False,False,ratelog)\n",
    "    return modelResults_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y  in ys:\n",
    "    y_train=y[0]\n",
    "    y_test=y[1]\n",
    "    ratelogFlag=y[2]\n",
    "    for m in models:\n",
    "        modelResults_df = ModelFit_cvect_exp_m_scaled(m,modelResults_df,X_cvect_exp_m_scaled_train,y_train,X_cvect_exp_m_scaled_test,y_test,ratelogFlag)\n",
    "        modelResults_df = ModelFit_tfidf_exp_m_scaled(m,modelResults_df,X_tfidf_exp_m_scaled_train,y_train,X_tfidf_exp_m_scaled_test,y_test,ratelogFlag)\n",
    "        modelResults_df = ModelFit_hashvect_exp_m_scaled(m,modelResults_df,X_hashvect_exp_m_scaled_train,y_train,X_hashvect_exp_m_scaled_test,y_test,ratelogFlag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем отличаются словари теста и трейна? Почему на тесте такая низкая точность?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформирую словарь для теста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_test = CountVectorizer(max_df=cv_max_df, min_df=cv_min_df,\\\n",
    "                             ngram_range=cv_ngram_range)\n",
    "tfidf_transformer_test = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_curr = X_test['ru_words_'+columnOne]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cvect_ru_words_test2 =count_vect_test.fit_transform(X_test_curr) \n",
    "X_tfidf_ru_words_test2 =tfidf_transformer_test.fit_transform\\\n",
    "                        (X_cvect_ru_words_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rr_ru_test = dict(zip(count_vect_test.get_feature_names(),\\\n",
    "                      tfidf_transformer_test.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_ru_test_df = token_weight_df(rr_ru_test,'ru')\n",
    "rr_ru_train_df = token_weight_df(rr_ru,'ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_ru_test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_ru_train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим вообще сколько слов есть в словаре трейна из тестовой выборки и какие у них веса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(rr_ru_train_df['ru']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(rr_ru_test_df['ru']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_ru_testIntrain = list(set(rr_ru_train_df['ru']) - set(rr_ru_test_df['ru']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rr_ru_testIntrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Практически все слова теста содержатся в словаре трейна. Значит модель плохо обучилась. Нет взаимосвязи между словами и рейтингом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробую сделать отдельные словари для отрицательных, нулевых и положительных статей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_rate = CountVectorizer(max_df=cv_max_df, min_df=cv_min_df,\\\n",
    "                             ngram_range=cv_ngram_range)\n",
    "tfidf_transformer_rate = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test_curr_0 = df[df['rate_category'] == 0]['ru_words_'+columnOne]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_curr_gt0 = df[df['rate_category'] > 0]['ru_words_'+columnOne]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_curr_lt0 = df[df['rate_category'] < 0]['ru_words_'+columnOne]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voc_for_rate (words):    \n",
    "    X_cvect_ru_words_rate =count_vect_rate.fit_transform(words) \n",
    "    X_tfidf_ru_words_rate =tfidf_transformer_rate.fit_transform(X_cvect_ru_words_rate)\n",
    "    rr_ru_rate = dict(zip(count_vect_rate.get_feature_names(),tfidf_transformer_rate.idf_))\n",
    "    return token_weight_df(rr_ru_rate,'ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_ru_rate_0 =  voc_for_rate(X_test_curr_0)\n",
    "rr_ru_rate_gt0 =  voc_for_rate(X_test_curr_gt0)\n",
    "rr_ru_rate_lt0 =  voc_for_rate(X_test_curr_lt0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('len_0={0}; len_gt0={1}; len_lt0={2}'.format(len(rr_ru_rate_0['ru']),len(rr_ru_rate_gt0['ru']),len(rr_ru_rate_lt0['ru'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сколько слов из статей с отрицательным рейтингом содержется в статьях с положительным:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rr_ru_rate_gt0['ru']) - len(list(set(rr_ru_rate_gt0['ru']) - set(rr_ru_rate_lt0['ru'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_ru_rate_gt0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_ru_rate_lt0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Практически все слова из статей с отрицательным рейтингом содержатся в статьях с положительным.\n",
    "Нужно проанализировать тексты этих статей и подмумать над передачей модели смысловых нагрузок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['ru_words_text_w2v'] = df['ru_words_text'].\\\n",
    "                                apply(lambda t: t.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = word2vec.Word2Vec(df['ru_words_text_w2v'], size=300, window=5, workers=2)\n",
    "w2v_dict = dict(zip(w2v.wv.index2word, w2v.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(positive=['гугл'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mean_vectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = len(next(iter(w2v_dict.values())))\n",
    "\n",
    "    def fit(self, X):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = mean_vectorizer(w2v).fit(df['ru_words_text_w2v']).\\\n",
    "                                    transform(df['ru_words_text_w2v'])\n",
    "data_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainTestSplitForW2V (coef,X,y):\n",
    "    \n",
    "    train_part_size = int(coef * X.shape[0])\n",
    "    \n",
    "    X_train, X_test = X[:train_part_size, :],X[train_part_size:, :]\n",
    "    y_train, y_test = y[:train_part_size],   y[train_part_size:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = TrainTestSplitForW2V(.8,data_mean,df['rate'])\n",
    "X_train, X_test, y_train_l, y_test_l = TrainTestSplitForW2V(.8,data_mean,df['rate_log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = [[y_train,y_test,False],[y_train_l,y_test_l,True]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['ru_words_text_w2v'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelFit_ru_words_w2v(m,modelResults_df,X_train,y_train,X_test,y_test,ratelog=False):\n",
    "    comment=columnOne+ '+ ru_words + w2v + rate_log:'+str(ratelog)\n",
    "    modelResults_df = ModelFitPredictAndMettrics(modelResults_df,comment,m,\\\n",
    "                      X_train,y_train,X_test,y_test,False,False,ratelog)\n",
    "    return modelResults_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y  in ys:\n",
    "    y_train=y[0]\n",
    "    y_test=y[1]\n",
    "    ratelogFlag=y[2]\n",
    "    for m in models:\n",
    "        modelResults_df = ModelFit_ru_words_w2v(m,modelResults_df,X_train,y_train,X_test,y_test,ratelogFlag) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelResults_df.sort_values(by='sMSETest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelResults_df.to_csv(fileNameFromExport, sep=';',index=False,encoding=\"windows-1251\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что дальше?\n",
    "1. Понять почему низкая оценка на тесте и повысить качество предикшена\n",
    "2. Попробовать n-граммы\n",
    "3. Разобраться с Pipeline и GridSearchCV, переписать с помощью них обучение и проработать разные гиперпараметры моделей\n",
    "4. По словарям, попробовать разные словари на положительные и отрицательные статьи, попробовать их объеденить взяв определенные части, понять чем они отличаются, какие уникальные вещи.\n",
    "5. kNN для построения мета-признаков\n",
    "6. Попробовать отдельные словари например на слова относящиеся к \"apple\", \"it\", \"деньгам\" и др."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
